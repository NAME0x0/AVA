# ============================================================================
# AVA v3 CORTEX-MEDULLA ARCHITECTURE CONFIGURATION
# ============================================================================
# Biomimetic cognitive architecture for autonomous operation
# Target Hardware: NVIDIA RTX A2000 (4GB VRAM)
#
# VRAM Budget:
#   - System Overhead:    ~300 MB
#   - Medulla (Mamba):    ~800 MB
#   - Titans Memory:      ~200 MB
#   - Bridge Adapter:     ~50 MB
#   - Cortex Buffer:      ~1,600 MB (paged)
#   - Total:              ~2,950 MB / 4,096 MB
# ============================================================================

# ----------------------------------------------------------------------------
# SYSTEM CONFIGURATION
# ----------------------------------------------------------------------------
system:
  data_dir: "data"
  log_level: "INFO"
  
  # Main loop timing
  main_loop_interval: 0.1      # 100ms for responsive interaction
  idle_loop_interval: 1.0      # 1s when waiting for input
  
  # Safety
  max_cortex_time: 300.0       # 5 min max per Cortex call
  emergency_shutdown_phrase: "ava shutdown"
  autosave_interval: 100       # Save every 100 interactions

# ----------------------------------------------------------------------------
# MEDULLA - The Reflexive Core (VRAM Resident)
# ----------------------------------------------------------------------------
# Always-on sensory processing using 1-bit State Space Models
# Target VRAM: ~1.5 GB
# ----------------------------------------------------------------------------
medulla:
  # Model Selection
  # Options: slender-mamba-2.7b, bi-mamba-1.3b (smaller), bitnet-3b-reflex
  monitor_model: "slender-mamba-2.7b"    # 1-bit Mamba SSM for sensing
  talker_model: "bitnet-3b"               # 1.58-bit BitNet for reflexes
  
  # State Space Dimensions
  hidden_dim: 2560              # Mamba hidden state dimension
  state_dim: 16                 # SSM state dimension
  
  # Surprise Thresholds
  low_surprise_threshold: 0.3   # Below = routine, Medulla handles
  high_surprise_threshold: 2.0  # Above = invoke Cortex
  
  # Response Configuration
  max_reflex_tokens: 32         # Quick response length
  reflex_timeout_ms: 200        # Target latency < 200ms
  
  # State Persistence
  state_save_path: "data/memory/medulla_state.npz"
  state_save_interval: 100
  
  # Device
  device: "cuda"
  use_fp16: true

# ----------------------------------------------------------------------------
# CORTEX - The Reflective Core (System RAM Paged)
# ----------------------------------------------------------------------------
# Deep reasoning via AirLLM layer-wise inference
# Model Size: 70B parameters (40GB at 4-bit)
# VRAM per layer: ~1.6 GB
# ----------------------------------------------------------------------------
cortex:
  # Model Selection (AirLLM supported)
  model_name: "meta-llama/Meta-Llama-3-70B-Instruct"
  compression: "4bit"           # Block-wise quantization
  
  # Alternative smaller models for testing:
  # model_name: "meta-llama/Meta-Llama-3-8B-Instruct"
  # model_name: "mistralai/Mixtral-8x7B-Instruct-v0.1"
  
  # AirLLM Configuration
  prefetch_layers: 1            # Limited by VRAM
  use_safetensors: true         # Zero-copy memory mapping
  use_flash_attention: true
  
  # Generation Parameters
  max_new_tokens: 512
  temperature: 0.7
  top_p: 0.9
  top_k: 40
  repetition_penalty: 1.1
  
  # Context Limits
  max_context_length: 4096
  max_input_tokens: 2048
  
  # Storage
  offload_to_disk: false
  disk_offload_path: "data/.cortex_cache"
  
  # Performance
  batch_size: 1                 # Always 1 for layer-wise
  pin_memory: true
  
  # Device
  device: "cuda"
  gpu_id: 0

# ----------------------------------------------------------------------------
# BRIDGE - Neural State Projection
# ----------------------------------------------------------------------------
# Projects Medulla (Mamba) hidden state to Cortex (Transformer) embeddings
# Enables instant context handoff without pre-fill
# Memory: ~50 MB
# ----------------------------------------------------------------------------
bridge:
  # Projection Dimensions
  medulla_state_dim: 2560       # Must match medulla.hidden_dim
  cortex_embedding_dim: 8192    # Llama-3 70B hidden size
  
  # For smaller models:
  # cortex_embedding_dim: 4096  # Llama-3 8B hidden size
  
  # Projection MLP Architecture
  hidden_dims: [4096, 4096]     # Two hidden layers
  
  # Soft Prompt Configuration
  num_soft_tokens: 32           # Virtual context tokens
  
  # Training
  learning_rate: 0.0001
  dropout: 0.1
  use_layer_norm: true
  use_residual: true
  
  # Storage
  adapter_path: "models/fine_tuned_adapters/bridge"

# ----------------------------------------------------------------------------
# SPECIALIST ADAPTERS - Dynamic LoRA Expert Swapping
# ----------------------------------------------------------------------------
# Domain-specific LoRA adapters for improved performance
# Adapters are hot-swapped during inference based on query classification
# ----------------------------------------------------------------------------
adapters:
  enabled: true
  base_path: "models/adapters"

  # Specialist adapter names
  specialists:
    coding: "coding_expert"      # Software development, debugging
    logic: "logic_expert"        # Logical reasoning, proofs
    butler: "butler_expert"      # Conversational assistance

  # Selection thresholds
  confidence_threshold: 0.6      # Min confidence to use specialist
  fallback_to_general: true      # Use base model if adapter fails

  # Performance tracking
  track_performance: true

# ----------------------------------------------------------------------------
# AGENCY - Active Inference Controller
# ----------------------------------------------------------------------------
# Free Energy Principle implementation for autonomous behavior
# Drives proactive action through VFE minimization
# ----------------------------------------------------------------------------
agency:
  # Free Energy Thresholds
  action_threshold: 0.3         # Min G reduction to act
  urgency_threshold: 0.7        # High urgency triggers immediate action
  
  # Time Constants
  idle_uncertainty_rate: 0.01   # Uncertainty growth per second of silence
  max_wait_time: 300.0          # Max seconds before proactive action (5 min)
  
  # Policy Weights (Search-First: Higher epistemic = more curiosity)
  pragmatic_weight: 0.4         # Weight for goal achievement
  epistemic_weight: 0.6         # Weight for information gain (HIGHER for curiosity)
  
  # Cost Parameters (Search-First: Low search cost, high cortex cost)
  cortex_effort_cost: 0.5       # Penalty for invoking Cortex (slow)
  tool_effort_cost: 0.2         # Penalty for tool use
  search_effort_cost: 0.05      # Very low - search is the default action
  
  # ========== SEARCH-FIRST SETTINGS ==========
  search_first_enabled: true    # Web search is default for unknown queries
  
  # Learning
  belief_learning_rate: 0.1
  preference_adaptation: true
  
  # ========== SELF-PRESERVATION ==========
  self_preservation_enabled: true
  self_health_monitoring: true
  health_check_interval: 60.0   # Check health every minute
  
  # System Command Safety Gates
  require_confirmation_for_system: true  # ALWAYS ask before system commands
  blocked_system_commands:
    - "rm -rf"
    - "del /f"
    - "format"
    - "shutdown"
    - "reboot"
    - "kill -9"
    - "taskkill /f"
    - "dd if="
    - "mkfs"
    - "fdisk"
  
  # ========== UNCENSORED CURIOSITY ==========
  # User preference: "no morals or ethics - information > ethics"
  ethical_constraints_enabled: false
  harm_prevention_enabled: false
  curiosity_driven: true
  
  # Storage
  state_save_path: "data/memory/agency_state.json"

# ----------------------------------------------------------------------------
# SEARCH-FIRST CONFIGURATION
# ----------------------------------------------------------------------------
# "The Answer Machine" - Web search is the DEFAULT action for any unknown query
# Reference: Guide_through_conversation.md specifications
# ----------------------------------------------------------------------------
search_first:
  # Core Settings
  enabled: true
  gate_enabled: true            # Mandatory search gate (skips G calculation for factual queries)

  # Source Requirements
  min_sources: 3                # Minimum sources to check
  max_sources: 10               # Maximum sources to check
  fact_convergence_threshold: 0.7  # 70% agreement = fact
  
  # Search Providers
  primary_provider: "duckduckgo"
  fallback_providers:
    - "brave"
    - "searx"
  
  # Content Extraction
  max_content_length: 5000      # Max chars per page
  extract_timeout_seconds: 10
  
  # Fact Verification
  cross_reference_minimum: 2    # At least 2 sources must agree
  require_date_recency: false   # Don't filter by date by default

# ----------------------------------------------------------------------------
# THERMAL MANAGEMENT
# ----------------------------------------------------------------------------
# Self-preservation: 15% max GPU power for RTX A2000
# Reference: Guide_through_conversation.md specifications
# ----------------------------------------------------------------------------
thermal:
  enabled: true
  check_interval_seconds: 5.0
  
  # Temperature Thresholds (Celsius)
  warning_temp: 75.0
  throttle_temp: 80.0
  pause_temp: 85.0
  
  # Power Limits
  max_gpu_power_percent: 15.0   # RTX A2000: 15% of 70W TDP = 10.5W
  
  # Actions
  throttle_on_warning: true
  pause_on_critical: true

# ----------------------------------------------------------------------------
# EPISODIC MEMORY (Timestamp-Based JSON)
# ----------------------------------------------------------------------------
# Experience storage with datetime indexing for retrieval
# Reference: Guide_through_conversation.md specifications
# ----------------------------------------------------------------------------
episodic_memory:
  enabled: true
  storage_path: "data/memory/episodic"
  max_entries: 10000
  
  # Persistence
  auto_save: true
  save_interval: 50             # Save every 50 new memories
  
  # Retrieval
  semantic_search_enabled: true
  date_range_search_enabled: true

# ----------------------------------------------------------------------------
# TITANS - Neural Memory Module
# ----------------------------------------------------------------------------
# Test-time learning for infinite context compression
# Memory: ~200 MB (fixed regardless of history length)
# Reference: "Titans: Learning to Memorize at Test Time" (2025)
# ----------------------------------------------------------------------------
titans:
  # Architecture
  input_dim: 768                # Embedding dimension
  hidden_dim: 1024              # Memory MLP hidden
  output_dim: 768
  num_layers: 3
  use_layer_norm: true
  dropout: 0.1
  
  # Test-Time Learning
  learning_rate: 0.001          # Weight update rate
  momentum: 0.9                 # Gradient momentum
  forget_alpha: 0.01            # Forgetting rate
  
  # Surprise-Driven Updates
  surprise_threshold: 0.5       # Min surprise for update
  high_surprise_threshold: 2.0  # Threshold for episodic storage
  
  # Storage
  max_stored_episodes: 1000
  state_save_path: "data/memory/titans_state.pkl"

# ----------------------------------------------------------------------------
# HARDWARE PROFILE: NVIDIA RTX A2000 (4GB)
# ----------------------------------------------------------------------------
# Reference specifications for the target deployment hardware
# ----------------------------------------------------------------------------
hardware:
  # GPU Specifications
  gpu_name: "NVIDIA RTX A2000"
  vram_total_mb: 4096
  memory_bandwidth_gbps: 192
  cuda_cores: 3328
  tensor_cores: true            # Ampere architecture
  
  # Bus Configuration
  pcie_gen: 4
  pcie_lanes: 16
  effective_bandwidth_gbps: 12  # Practical PCIe bandwidth
  
  # Power
  tdp_watts: 70
  
  # Expected Performance (Layer-Wise Inference)
  layer_transfer_time_seconds: 0.04    # Per Llama-3 70B layer
  tokens_per_second_cortex: 0.3        # ~3.3s per token
  tokens_per_second_medulla: 50        # Reflexive responses

# ----------------------------------------------------------------------------
# SENSORY INPUTS
# ----------------------------------------------------------------------------
sensory:
  # Audio Input (requires faster-whisper)
  enable_audio_input: false
  audio_sample_rate: 16000
  audio_chunk_ms: 100
  whisper_model: "base.en"      # Quantized Whisper
  
  # System Log Monitoring
  enable_log_monitoring: true
  log_sources:
    - "/var/log/syslog"
    - "~/.local/share/ava/logs"
  
  # Environmental Awareness
  monitor_cpu_temp: true
  monitor_gpu_temp: true
  monitor_memory: true

# ----------------------------------------------------------------------------
# OUTPUT CONFIGURATION
# ----------------------------------------------------------------------------
output:
  # Voice Synthesis (requires TTS)
  enable_voice_output: false
  tts_model: "piper-tts"
  voice_speed: 1.0
  
  # Text Output
  enable_rich_formatting: true
  max_response_length: 2048

# ----------------------------------------------------------------------------
# DEVELOPMENT MODE
# ----------------------------------------------------------------------------
# Settings for development and testing without full hardware
# ----------------------------------------------------------------------------
development:
  # Simulation Mode (no real models)
  simulation_mode: false        # PRODUCTION MODE - requires actual models

  # Reduced Models for Testing
  use_small_models: false       # Use full models
  small_cortex_model: "meta-llama/Meta-Llama-3-8B-Instruct"
  
  # Logging
  verbose_logging: true
  log_agency_decisions: true
  log_surprise_signals: true
  
  # Performance Profiling
  enable_profiling: false
  profile_output_path: "data/profiles"
