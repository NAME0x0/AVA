# AVA Project: Key Assumptions for Local Agentic AI (RTX A2000 4GB)

**Note:** The assumptions outlined in this document pertain to the development of AVA as a compact, powerful, and locally-run agentic AI model on an **NVIDIA RTX A2000 GPU with a strict 4GB VRAM limit**. These assumptions are critical for the feasibility of the project as detailed in the main `[ARCHITECTURE.md](./ARCHITECTURE.md)`.

---

The design and development of AVA are built upon several key assumptions regarding hardware capabilities, software optimizations, and model architectures.

## I. Hardware Assumptions (NVIDIA RTX A2000 4GB)

1.  **Specified VRAM is Absolute:** The 4GB GDDR6 VRAM is treated as a hard, unyielding constraint. All model and software choices must fit within this budget during inference and, as much as possible, during fine-tuning.
2.  **Ampere Architecture Capabilities:** The RTX A2000's Ampere architecture, with its 3rd Gen Tensor Cores, provides adequate support for:
    *   **Efficient Quantization:** Compatibility with INT4/NF4/FP4 quantization techniques (e.g., via `bitsandbytes`).
    *   **Mixed-Precision Operations:** Support for TF32, BFloat16, which can be leveraged by optimization libraries.
3.  **PCIe Gen 4 Throughput:** Sufficient data transfer speed between system RAM and GPU VRAM for scenarios where parts of the model or data might be offloaded (though direct VRAM operation is prioritized).
4.  **Sufficient System RAM & CPU:** While the GPU is the bottleneck, it is assumed the host system has adequate CPU power and system RAM (e.g., 16GB+) to support the OS, AVA's core logic, UIs, and any data processing that occurs outside the GPU.

## II. Software & Model Architecture Assumptions

1.  **Effectiveness of 4-bit Quantization:** Assumes that 4-bit quantization (e.g., NF4) can reduce model size by approximately 75% (vs. FP16) with acceptable and manageable degradation in performance for the chosen base models. Libraries like `bitsandbytes` are assumed to effectively implement this.
2.  **Viability of Ultra-Small Base Models:** Assumes that models in the 1B to 4B parameter range (e.g., Gemma 3n series) possess sufficient foundational capabilities to be specialized into effective agents after quantization and fine-tuning.
3.  **Efficacy of Parameter-Efficient Fine-Tuning (PEFT):**
    *   **QLoRA Feasibility:** Assumes QLoRA allows for effective fine-tuning of the 4-bit quantized model primarily within the 4GB VRAM, by only updating a small number of adapter weights.
    *   **Knowledge Retention:** Assumes QLoRA largely prevents catastrophic forgetting of pre-trained knowledge.
4.  **Impact of Knowledge Distillation:** Assumes that knowledge distillation can successfully transfer specific, advanced capabilities from larger "teacher" models to the compact AVA "student" model, allowing it to become "more advanced" in targeted areas.
5.  **Synthetic Data Utility:** Assumes that high-quality synthetic data generated by other LLMs can be effectively used to train AVA for specialized agentic tasks (reasoning, tool use, structured output), overcoming scarcity of human-labeled data for such niches.
6.  **Function Calling & Structured Output Reliability:** Assumes that the chosen base model, after PEFT, can be reliably trained to:
    *   Detect the need for external tools/functions.
    *   Generate well-formed structured arguments (e.g., JSON) for these calls.
    *   Produce consistent structured outputs as required by the agentic framework.
7.  **Model Context Protocol (MCP) Practicality:** Assumes that MCP can provide a more efficient and secure alternative to traditional RAG for real-time data access, and that lightweight MCP servers can be run locally or accessed with minimal overhead.
8.  **Local Server & UI Performance:** Assumes that local LLM servers (like Ollama or a custom Python server) and GUIs (like Open WebUI) can run efficiently on the user's machine without consuming excessive resources that would impede AVA's performance.
9.  **Secure Tunneling Viability:** Assumes that services like `Localtonet` or `ngrok` can provide secure and reasonably performant remote access to AVA's local instance.

## III. Development & User Assumptions

1.  **Technical Proficiency of User/Developer:** The initial setup, fine-tuning, and advanced customization of AVA assume a user with a good degree of technical proficiency (comfortable with CLI, Python, AI concepts).
2.  **Iterative Development:** The project assumes an iterative development approach, where capabilities are built and refined incrementally, and performance is continuously monitored and optimized.
3.  **Availability of Supporting Tools & Libraries:** Relies on the continued availability and development of key open-source libraries (`transformers`, `bitsandbytes`, `peft`, `trl`, `unsloth`, `ollama`, `Open WebUI`, etc.).

Failure or significant deviation in any of these assumptions would necessitate a re-evaluation of AVA's architecture, scope, or feasibility on the target hardware. The project aims to actively validate these assumptions during development.
