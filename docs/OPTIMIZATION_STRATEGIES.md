# AVA Optimization Strategies ⚙️

**Project Vision:** To establish AVA as a compact, powerful, and locally-run agentic AI model capable of serving as a daily driver for advanced tasks on an NVIDIA RTX A2000 GPU with 4GB VRAM.

This document details the core optimization strategies employed in Project AVA to achieve high performance and advanced capabilities within significant hardware constraints. It covers model compression techniques, efficient fine-tuning, and data strategies.

**Related Documents:**
*   `[ARCHITECTURE.md](./ARCHITECTURE.md)`: Overall system design.
*   `[AGENTIC_DESIGN.md](./AGENTIC_DESIGN.md)`: Agentic capabilities.
*   `[ROADMAP.md](./ROADMAP.md)`: Development phases.
*   `[ADVANCED_TECHNIQUES.md](./ADVANCED_TECHNIQUES.md)`: Research and future optimizations.

---

## I. Core Challenge: 4GB VRAM Limitation

The NVIDIA RTX A2000 with its 4GB VRAM is the primary constraint. All strategies aim to maximize model capability while fitting within this strict memory budget.

## II. Model Compression Techniques

### A. Aggressive 4-bit Quantization (INT4/NF4/FP4)

*   **Objective:** Significantly reduce model size (weights and activations) with manageable performance degradation.
*   **Methodology:**
    *   Utilize libraries like `bitsandbytes` for 4-bit quantization (NormalFloat4 - NF4, or FloatPoint4 - FP4).
    *   Explore `LLM.int8()` for 8-bit as a fallback or for specific layers if beneficial.
    *   Focus on techniques like double quantization and paged optimizers provided by `bitsandbytes`.
*   **Implementation Details:** (To be filled with specifics of quantization scripts, parameters used, and chosen data types like `torch.bfloat16` for computation.)
    *   See `scripts/quantize_model.py` for an example implementation.
*   **Expected Outcome:** ~75% reduction in model size compared to FP16, enabling larger base models to fit into VRAM.
*   **Evaluation:** Perplexity, task-specific benchmarks, inference speed, and VRAM footprint post-quantization.

### B. Knowledge Distillation

*   **Objective:** Transfer capabilities from a larger, more powerful "teacher" model to the compact AVA "student" model.
*   **Methodology:**
    *   Train AVA on the soft probabilities (logits) or intermediate representations of a teacher model.
    *   Focus on distilling specific agentic skills or domain knowledge.
*   **Teacher Model Candidates:** (To be identified - e.g., larger open-source models or API-based models like GPT-3.5/4 for initial data generation).
*   **Data Sources:** Original task-specific datasets, or synthetic data generated by the teacher.
*   **Implementation Details:** (To be filled with distillation pipeline specifics, loss functions, and training regime.)
*   **Expected Outcome:** Enhanced performance on targeted tasks for AVA, surpassing what it could achieve through direct fine-tuning alone.

### C. Pruning & Sparsification (Research & Potential Implementation)

*   **Objective:** Further reduce model size by removing redundant parameters (weights, neurons, layers).
*   **Methodology (Exploratory):**
    *   **Structured Pruning:** Removing entire blocks of weights.
    *   **Unstructured Pruning (e.g., SparseGPT):** Setting individual weights to zero.
    *   Potential for ONNX export for optimized inference with sparse models.
*   **Implementation Details:** (To be filled if pursued, including choice of pruning technique, sparsity levels, and retraining strategies.)
*   **Expected Outcome:** Additional model size reduction with minimal impact on critical capabilities.

## III. Parameter-Efficient Fine-Tuning (PEFT)

### A. QLoRA (Quantized Low-Rank Adaptation)

*   **Objective:** Efficiently fine-tune the 4-bit quantized AVA model for specific agentic tasks with minimal VRAM overhead during training.
*   **Methodology:**
    *   Freeze the majority of the pre-trained 4-bit quantized LLM weights.
    *   Introduce a small set of trainable Low-Rank Adaptation (LoRA) weights (adapters).
    *   Utilize libraries like Hugging Face `peft`, `trl`, and `unsloth` (for speed and memory optimization).
*   **Implementation Details:**
    *   Target LoRA modules (e.g., attention layers like `q_proj`, `v_proj`).
    *   Configuration of LoRA parameters (rank `r`, `lora_alpha`, dropout).
    *   Training regime, learning rates, and batch sizes optimized for 4GB VRAM.
    *   See `notebooks/02_qlora_finetuning_example.ipynb` for a conceptual example.
*   **Expected Outcome:** Effective specialization of AVA for desired tasks (function calling, structured output) without catastrophic forgetting and within VRAM limits.

## IV. High-Quality Synthetic Dataset Generation

*   **Objective:** Create diverse, high-quality instruction-following datasets tailored for AVA's agentic tasks, especially where human-labeled data is scarce.
*   **Methodology:**
    *   Leverage larger LLMs (e.g., GPT-3.5/4 via API, or capable open-source models) to generate instruction-response pairs, reasoning chains, or tool-use examples.
    *   Employ sophisticated prompting techniques (few-shot, role-playing, template-based generation).
    *   Iterative refinement: Generate data, evaluate its quality, refine prompts, and regenerate.
*   **Tools & Techniques:**
    *   Custom scripts (e.g., `scripts/generate_synthetic_data.py`).
    *   Potentially explore tools like Gretel Navigator for specific data generation needs.
*   **Data Quality Control:** Filtration, deduplication, and human review of samples to ensure accuracy and relevance.
*   **Implementation Details:** (To be filled with specific prompt templates, generation scripts, and quality control processes.)
    *   Example synthetic datasets will be stored in `data/synthetic_datasets/`.
*   **Expected Outcome:** A rich source of training data for QLoRA fine-tuning, enabling AVA to learn complex agentic behaviors.

## V. Ongoing Benchmarking & Iteration

All optimization strategies will be subject to continuous benchmarking:
*   **VRAM Usage:** During idle, inference, and (if applicable locally) fine-tuning.
*   **Inference Latency:** Speed of response generation.
*   **Task Performance:** Accuracy and success rates on relevant agentic benchmarks or custom evaluation sets.
*   **Qualitative Analysis:** Human evaluation of response coherence, helpfulness, and tool usage.

Results will inform iterative adjustments to quantization parameters, LoRA configurations, synthetic data generation prompts, and overall model architecture choices. 

---

## 4-bit Model Quantization with bitsandbytes

To enable running capable models on consumer hardware with limited VRAM, such as the NVIDIA RTX A2000 (4GB), we employ 4-bit quantization. This significantly reduces the memory footprint of the language model.

### Method and Configuration

We utilize the `bitsandbytes` library integrated with Hugging Face `transformers` to perform post-training quantization. The primary configuration parameters used are:

-   **`load_in_4bit=True`**: This flag enables the loading of model weights in 4-bit precision.
-   **`bnb_4bit_quant_type="nf4"`**: Specifies the "NormalFloat4" (NF4) quantization type, which is a sophisticated method designed to handle normally distributed weights effectively.
-   **`bnb_4bit_use_double_quant=True`**: Enables a nested quantization scheme where quantization constants are themselves quantized, further saving memory.
-   **`bnb_4bit_compute_dtype=torch.bfloat16`**: Sets the computation data type to `bfloat16`. While weights are stored in 4-bit, computations (like matrix multiplications during inference) are performed in a higher precision (e.g., `bfloat16` or `float16`) for accuracy. `bfloat16` is often preferred on modern GPUs that support it.

### Target Model

The initial target for this quantization is the **Gemma 3n 4B** model (currently using `google/gemma-2b` as a placeholder in the scripts due to its availability and suitability for the VRAM target).

### Implementation Script: `scripts/quantize_model.py`

The core of the quantization process is handled by the enhanced `scripts/quantize_model.py`. This script offers a comprehensive solution for model quantization, including:

-   **Command-Line Interface:** Allows specifying model ID, output path, quantization strategy (e.g., `bnb_4bit`, `bnb_8bit`), quantization type (e.g., `nf4`, `fp4`), VRAM limits, and other parameters.
-   **Configuration Management:** Uses dataclasses for detailed control over quantization settings.
-   **VRAM Optimization:** Includes memory management features specifically for environments like the RTX A2000.
-   **Model Analysis:** Extracts and logs information about the model being quantized.
-   **Built-in Validation:** Can perform a test generation after quantization to verify model integrity using the `--test_generation` flag. It also includes a `--test` flag to run a full test pipeline with a small predefined model (e.g., `microsoft/DialoGPT-small`).
-   **Metadata Generation:** Saves a `quantization_metadata.json` file alongside the quantized model, detailing the model info, quantization configuration, and performance metrics like VRAM usage.
-   **Error Handling & Logging:** Provides robust logging and error management.

**Basic Usage Example:**
```bash
python scripts/quantize_model.py \
    --model_id google/gemma-2b \
    --output_path ./models/quantized/gemma-2b-4bit \
    --strategy bnb_4bit \
    --quantization_type nf4 \
    --test_generation
```
For a quick test of the script and environment:
```bash
python scripts/quantize_model.py --test
```

### Expected Outcome

The primary goal is to reduce the model's VRAM usage to comfortably fit within 4GB, allowing for local inference on the target hardware. While some minor degradation in performance metrics is expected compared to the full-precision model, 4-bit quantization with NF4 is known to maintain a high degree of model accuracy.

### VRAM Usage & Performance Notes

*(To be updated after running `scripts/quantize_model.py` on the target hardware. The script's output and the generated `quantization_metadata.json` will provide key metrics.)*

-   **Base Model Estimated Size (MB):** The script logs this.
-   **Quantized Model Size (MB):** The script logs this and it can be derived from saved files.
-   **Compression Ratio:** Calculated and logged by the script.
-   **VRAM Usage (GB) during quantization:** Measured and logged by the script.
-   **Quantization Time (seconds):** Measured and logged by the script.
-   **Qualitative Observations from Test Generation:** Based on output from the `--test_generation` flag.

---
