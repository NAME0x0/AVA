# AVA - Developmental AI
# An AI that learns and grows like a human child
# Dependencies for developmental, emotional, memory, and learning systems

# ============================================================================
# Core Dependencies
# ============================================================================
python-dateutil>=2.8.0
pyyaml>=6.0

# ============================================================================
# LLM Integration (Ollama)
# ============================================================================
httpx>=0.27.0,<1.0.0  # Async HTTP client for Ollama API

# ============================================================================
# HTTP Server (aiohttp for API server)
# ============================================================================
aiohttp>=3.9.0,<4.0.0  # Async HTTP server for AVA API

# ============================================================================
# CLI & User Interface
# ============================================================================
typer>=0.9.0,<1.0.0
rich>=13.7.0,<14.0.0  # Enhanced CLI output and formatting

# ============================================================================
# Fine-Tuning & Learning (Optional - for QLoRA training)
# ============================================================================
# Uncomment these if you want to run local fine-tuning:
# torch>=2.1.0
# transformers>=4.38.0,<5.0.0
# peft>=0.8.0,<1.0.0
# bitsandbytes>=0.42.0
# accelerate>=0.25.0,<1.0.0
# datasets>=2.16.0,<3.0.0

# ============================================================================
# Native Model Loading (Optional - for non-Ollama backends)
# ============================================================================
# AirLLM: Layer-wise inference for 70B+ models on limited VRAM
# Uncomment if using backend: "native" or "hybrid" in config
# airllm>=2.10.0

# Mamba SSM: State Space Models for O(N) inference
# Requires CUDA - uncomment if using native Medulla models
# mamba-ssm>=2.2.0
# causal-conv1d>=1.4.0

# llama-cpp-python: CPU/GPU GGUF quantized model inference
# Alternative to BitNet, supports 1-bit to 8-bit quantization
# llama-cpp-python>=0.3.0

# ============================================================================
# Core Dependencies
# ============================================================================
pydantic>=2.5.0,<3.0.0  # Data validation and structured output
sse-starlette>=1.8.0,<2.0.0  # Server-sent events for token streaming

# ============================================================================
# Data Handling & Processing
# ============================================================================
pandas>=2.1.0,<3.0.0
numpy>=1.24.0,<2.0.0
PyYAML>=6.0.1,<7.0.0
python-dotenv>=1.0.0,<2.0.0

# ============================================================================
# LlamaIndex Integration (Structured Output & MCP concepts)
# ============================================================================
llama-index>=0.10.0,<1.0.0
llama-index-core>=0.10.0,<1.0.0
llama-index-llms-ollama>=0.1.0,<1.0.0

# ============================================================================
# Async Support & Performance
# ============================================================================
asyncio-throttle>=1.0.2,<2.0.0
aiofiles>=23.2.0,<24.0.0

# ============================================================================
# Security & Authentication (for remote access)
# ============================================================================
passlib[bcrypt]>=1.7.4,<2.0.0
python-jose[cryptography]>=3.3.0,<4.0.0
python-multipart>=0.0.6,<1.0.0
ecdsa>=0.19.0  # Pinned to fix CVE-2024-23342 timing attack vulnerability

# ============================================================================
# Development & Testing Dependencies
# ============================================================================
pytest>=7.4.0,<8.0.0
pytest-asyncio>=0.23.0,<1.0.0
pytest-cov>=4.1.0,<5.0.0

# Code quality and formatting
ruff>=0.1.15,<1.0.0  # Fast Python linter and formatter
black>=23.12.0,<25.0.0
mypy>=1.8.0,<2.0.0

# ============================================================================
# Terminal User Interface (TUI)
# ============================================================================
textual>=0.40.0,<1.0.0  # Modern TUI framework with CSS-like styling

# ============================================================================
# Optional Dependencies (uncomment as needed)
# ============================================================================
# jupyter>=1.0.0,<2.0.0  # For notebooks
# matplotlib>=3.7.0,<4.0.0  # For visualizations
# seaborn>=0.13.0,<1.0.0  # Enhanced plotting
# scikit-learn>=1.3.0,<2.0.0  # For additional ML utilities

# ============================================================================
# AVA v3 Cortex-Medulla Architecture (Advanced)
# ============================================================================
# These dependencies enable the full Cortex-Medulla architecture
# Requires NVIDIA GPU with CUDA support

# AirLLM - Layer-wise inference for large models on limited VRAM
# Enables running 70B models on 4GB GPU via layer paging
# pip install airllm

# Mamba SSM - State Space Models for O(N) inference
# Enables always-on processing without KV cache growth
# pip install mamba-ssm
# pip install causal-conv1d>=1.2.0

# BitNet - 1-bit LLM inference
# Extreme memory efficiency for the Medulla
# pip install bitnet-cpp  # When available
# Alternative: Use GGUF quantized models via llama-cpp-python

# Active Inference - Free Energy Principle implementation
# Drives autonomous behavior through VFE minimization
# pip install pymdp

# Titans Neural Memory - Test-time learning
# pip install titans-pytorch

# faster-whisper - Efficient speech recognition
# For audio input processing in the Medulla
# pip install faster-whisper

# ============================================================================
# Notes:
# ============================================================================
# 1. CUDA-specific PyTorch installation:
#    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
# 2. bitsandbytes may require OS-specific installation - see docs/INSTALLATION.md
# 3. Tunneling services (localtonet, ngrok) are external services, not Python packages
# 4. Version ranges allow for security updates while maintaining compatibility
# 5. Pin exact versions in production: pip freeze > requirements-lock.txt
