# AVA - Autonomous Virtual Assistant

<p align="center">
  <img src="docs/assets/ava_logo.png" alt="AVA Logo" width="150" />
</p>

<p align="center">
  <a href="https://github.com/NAME0x0/AVA/actions"><img src="https://img.shields.io/github/actions/workflow/status/NAME0x0/AVA/ci.yml?branch=main&style=flat-square&logo=github&label=CI" alt="CI Status"></a>
  <a href="https://github.com/NAME0x0/AVA"><img src="https://img.shields.io/badge/python-3.10%2B-blue?style=flat-square&logo=python&logoColor=white" alt="Python 3.10+"></a>
  <a href="https://github.com/NAME0x0/AVA/blob/main/LICENSE"><img src="https://img.shields.io/github/license/NAME0x0/AVA?style=flat-square" alt="License"></a>
  <a href="https://github.com/NAME0x0/AVA/releases"><img src="https://img.shields.io/github/v/release/NAME0x0/AVA?style=flat-square&include_prereleases" alt="Release"></a>
  <a href="https://github.com/NAME0x0/AVA/releases/latest"><img src="https://img.shields.io/github/downloads/NAME0x0/AVA/total?style=flat-square&logo=windows&label=Downloads" alt="Downloads"></a>
</p>

<p align="center">
  <a href="https://github.com/NAME0x0/AVA/releases/latest">
    <img src="https://img.shields.io/badge/Download-Latest%20Release-0078D4?style=for-the-badge&logo=windows&logoColor=white" alt="Download Latest Release">
  </a>
</p>

**AVA v4** is a research-grade AI assistant with a **biomimetic dual-brain architecture** inspired by the human nervous system. It runs locally on constrained hardware (4GB VRAM) and prioritizes accuracy over speed.

## What's New in v4

- **Unified Rust Backend**: Single portable executable with embedded HTTP server (no Python required)
- **Cortex-Medulla Architecture**: Fast reflexive responses for simple queries, deep reasoning for complex ones
- **Desktop App**: Native Tauri + Next.js GUI with real-time neural activity visualization
- **Active Inference Metrics**: Free Energy calculation and belief state visualization
- **System Tray**: Run in background with minimal resource usage
- **Terminal UI**: Power-user TUI built with Textual (streaming support coming soon)
- **Search-First Paradigm**: Web search as default for informational queries
- **Titans Neural Memory**: Infinite context through test-time learning
- **Active Inference**: Autonomous behavior using Free Energy Principle
- **Automated Bug Reporting**: One-click bug reports with system info

---

## Architecture

AVA v4 uses a **unified single-app architecture** for maximum portability:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AVA Desktop App (Single Executable)                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Embedded Rust Backend (Axum HTTP Server)               â”‚   â”‚
â”‚  â”‚  - All AI processing via Ollama                         â”‚   â”‚
â”‚  â”‚  - Runs on http://127.0.0.1:8085                        â”‚   â”‚
â”‚  â”‚  - Active Inference metrics calculation                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â”‚                                  â”‚
â”‚                              â”‚ Internal HTTP                    â”‚
â”‚                              â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Next.js Frontend                                        â”‚   â”‚
â”‚  â”‚  - Real-time neural activity visualization               â”‚   â”‚
â”‚  â”‚  - Metrics dashboard with Free Energy display            â”‚   â”‚
â”‚  â”‚  - Chat interface with streaming responses               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ HTTP API
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ollama (Local LLM Server)                                      â”‚
â”‚  - gemma3:4b (fast mode)                                       â”‚
â”‚  - llama3.2:latest (deep thinking mode)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Single executable** - no Python installation required for end users.

---

## Installation

### Prerequisites

1. **Ollama** - [Download Ollama](https://ollama.ai/) - **Required**
   ```bash
   ollama pull gemma3:4b
   ollama pull llama3.2:latest  # For deep thinking mode
   ollama serve
   ```

### Quick Start

**Option A: Download Pre-built App (Recommended)**

Download and run the desktop app from [Releases](https://github.com/NAME0x0/AVA/releases/latest).
- `AVA_4.0.0_x64-setup.exe` - Windows installer
- `AVA_4.0.0_x64_en-US.msi` - Windows MSI package

**Option B: Build from Source**

```bash
git clone https://github.com/NAME0x0/AVA.git
cd AVA/ui
npm install
npm run tauri build
```

**Option C: Development Mode**

```bash
cd AVA/ui
npm install
npm run tauri dev
```

### Windows Installer

<p align="center">
  <a href="https://github.com/NAME0x0/AVA/releases/latest">
    <img src="https://img.shields.io/badge/Download-Latest%20Release-28a745?style=for-the-badge&logo=github&logoColor=white" alt="Latest Release">
  </a>
</p>

The release includes:
- `AVA_4.0.0_x64-setup.exe` - Desktop app installer (single executable, no Python needed)
- `AVA_4.0.0_x64_en-US.msi` - Windows MSI package

---

## Quick Start

> **New to open source or AI projects?** See our [Beginner's Guide](docs/BEGINNER_GUIDE.md) for step-by-step instructions.

### Running AVA

**Desktop App (Recommended)**
```bash
# Download from Releases, or build from source:
cd ui
npm install
npm run tauri dev
```

**Terminal UI (Power Users)**
```bash
# Requires Python environment
pip install -e .
python -m tui.app
```

The TUI provides a keyboard-driven interface with:
- Real-time metrics display
- Command palette (Ctrl+K)
- Force modes (Ctrl+S for search, Ctrl+D for deep thinking)
- Settings management

---

## API Endpoints

The embedded server exposes these endpoints on `http://127.0.0.1:8085`:

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/health` | GET | Server health check |
| `/chat` | POST | Send message, get AI response |
| `/cognitive` | GET | Current cognitive state (entropy, surprise, varentropy) |
| `/memory` | GET | Memory statistics |
| `/belief` | GET | Active Inference belief state and free energy |
| `/stats` | GET | System statistics |

---

## Architecture

```
User Input
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MEDULLA (Reflexive Core) - Always On                  â”‚
â”‚  - Mamba SSM for O(1) memory sensing                   â”‚
â”‚  - 1-bit BitNet for quick responses                    â”‚
â”‚  - Calculates "surprise" signal                        â”‚
â”‚  - VRAM: ~800 MB (resident)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                              â”‚
    â”‚ Low Surprise                 â”‚ High Surprise
    â–¼                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Quick Reply  â”‚             â”‚  CORTEX (Reflective Core)   â”‚
â”‚(<200ms)     â”‚             â”‚  - 70B model via AirLLM     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚  - Layer-wise paging        â”‚
                            â”‚  - ~3.3s per token          â”‚
                            â”‚  - VRAM: ~1.6 GB (paged)    â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

| Component | Location | Purpose |
|-----------|----------|---------|
| **Medulla** | `src/core/medulla.py` | Always-on sensory processing |
| **Cortex** | `src/core/cortex.py` | Deep reasoning (70B on 4GB) |
| **Bridge** | `src/core/bridge.py` | Projects Medulla â†’ Cortex |
| **Agency** | `src/core/agency.py` | Active Inference |
| **Titans** | `src/hippocampus/titans.py` | Test-time learning |
| **System** | `src/core/system.py` | Orchestration |

---

## Implementation Status

> **Note**: This section clarifies what is currently implemented vs. planned for future development.

### âœ… Fully Implemented

| Feature | Description |
|---------|-------------|
| **Ollama Integration** | Full LLM inference via Ollama (gemma3:4b default) |
| **HTTP API Server** | REST + WebSocket endpoints for chat, tools, status |
| **Search-First Workflow** | Web search as default for informational queries |
| **Active Inference** | Autonomous policy selection using Free Energy Principle |
| **Entropy-Based Routing** | Query complexity analysis via Entropix |
| **Command Safety** | Blocking dangerous system commands |
| **Thermal Monitoring** | GPU temperature tracking and throttling |
| **Terminal UI** | Full-featured TUI with Textual |
| **Desktop GUI** | Tauri + Next.js with neural visualization |
| **Memory System** | Episodic memory with conversation storage |

### ğŸš§ Designed but Not Yet Implemented

| Feature | Description | Status |
|---------|-------------|--------|
| **AirLLM (70B)** | Layer-wise paging for large models | Architecture ready |
| **BitNet 3B** | 1.58-bit quantized Medulla talker | Not integrated |
| **Slender-Mamba** | 1-bit SSM for Medulla monitor | Not integrated |
| **Titans Test-Time Learning** | Online memory weight updates | Architecture ready |
| **Bridge Adapter Training** | MLP projection training pipeline | Not implemented |
| **Expert Adapters** | DeepSeek-Coder, Butler adapters | Not created |

---

## API Reference

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/health` | GET | Health check |
| `/chat` | POST | Send message (auto-routes based on force mode) |
| `/cognitive` | GET | Cognitive state (entropy, surprise, varentropy) |
| `/memory` | GET | Memory statistics |
| `/belief` | GET | Active Inference belief state and free energy |
| `/stats` | GET | System statistics |

### Using the API

```bash
# Health check
curl http://127.0.0.1:8085/health

# Send a message
curl -X POST http://127.0.0.1:8085/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "What is Python?"}'

# Get cognitive state
curl http://127.0.0.1:8085/cognitive

# Get belief state with free energy
curl http://127.0.0.1:8085/belief
```

---

## TUI Keybindings

| Key | Action |
|-----|--------|
| `Ctrl+K` | Command palette |
| `Ctrl+L` | Clear chat |
| `Ctrl+T` | Toggle metrics |
| `Ctrl+S` | Force search |
| `Ctrl+D` | Deep think |
| `F1` | Help |
| `Ctrl+Q` | Quit |

---

## Configuration

Main config: `config/cortex_medulla.yaml`

```yaml
development:
  simulation_mode: true  # For testing without models

search_first:
  enabled: true
  min_sources: 3
  agreement_threshold: 0.7

thermal:
  max_gpu_power_percent: 15  # RTX A2000 safe limit
  warning_temp_c: 75
  throttle_temp_c: 80

agency:
  epistemic_weight: 0.6  # High curiosity
```

---

## Project Structure

```
AVA/
â”œâ”€â”€ config/              # Configuration files
â”œâ”€â”€ data/                # Runtime data
â”œâ”€â”€ docs/                # Documentation
â”œâ”€â”€ installer/           # Installer build system
â”‚   â”œâ”€â”€ config/          # Installer configuration
â”‚   â”œâ”€â”€ nsis/            # NSIS scripts (Windows)
â”‚   â””â”€â”€ scripts/         # Build automation
â”œâ”€â”€ legacy/              # Archived Python server code
â”œâ”€â”€ models/              # Model adapters
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ava/             # Python API (for TUI/development)
â”‚   â”œâ”€â”€ core/            # Cortex-Medulla system
â”‚   â”œâ”€â”€ hippocampus/     # Titans memory
â”‚   â”œâ”€â”€ cortex/          # Utilities
â”‚   â”œâ”€â”€ inference/       # LLM inference
â”‚   â”œâ”€â”€ learning/        # QLoRA training
â”‚   â”œâ”€â”€ subconscious/    # Background processing
â”‚   â””â”€â”€ tools/           # Tool implementations
â”œâ”€â”€ tests/               # Test suite
â”œâ”€â”€ tui/                 # Terminal UI (Textual)
â””â”€â”€ ui/                  # Desktop GUI (Next.js + Tauri)
    â””â”€â”€ src-tauri/       # Rust backend (embedded server)
        â””â”€â”€ src/
            â”œâ”€â”€ main.rs      # Application entry point
            â””â”€â”€ engine/      # HTTP server (Axum)
```

---

## VRAM Budget (RTX A2000 4GB)

```
System Overhead:    300 MB
Medulla (Mamba):    800 MB
Titans Memory:      200 MB
Bridge Adapter:      50 MB
Cortex Buffer:    1,600 MB (paged on-demand)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total Resident:   2,050 MB
Peak (Cortex):    3,650 MB
Headroom:           446 MB
```

---

## Troubleshooting

### "Ollama is not running"
```bash
ollama serve
```

### "No models available"
```bash
ollama pull gemma3:4b
ollama pull llama3.2:latest
```

### "Port 8085 already in use"
```bash
# Windows
netstat -ano | findstr :8085
taskkill /F /PID <pid>

# Linux/macOS
lsof -i :8085
kill -9 <pid>
```

### Slow Responses
- First response is slower (model loading)
- Deep thinking (Cortex mode) takes 5-30 seconds
- Use simulation mode for testing without models

---

## Contributing

Contributions welcome! See [CONTRIBUTING.md](CONTRIBUTING.md).

---

## License

MIT License - see [LICENSE](LICENSE).

---

## Credits

Built with:
- [Ollama](https://ollama.ai/) - Local LLM inference
- [Textual](https://textual.textualize.io/) - TUI framework
- [Tauri](https://tauri.app/) - Desktop apps
- [Next.js](https://nextjs.org/) - React framework

Research papers:
- Titans (2025) - Test-time learning
- Entropix (2024) - Entropy-guided routing
- Active Inference - Free Energy Principle

---

<p align="center">
Made with care for the research community
</p>
