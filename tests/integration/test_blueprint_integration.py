"""
Integration Tests for Small Language Model Engineering Blueprint

These tests verify that the key components from the research blueprint
work correctly together:

1. TitansMemory: Surprise-based test-time memory updates
2. FastSlowWeightManager: Multi-timescale parameter updates
3. ToolformerParser: Autonomous tool-call parsing and augmentation
4. ChainOfThoughtEnforcer: CoT reasoning extraction
5. DistillationPipeline: Self-distillation loop

Reference Papers:
- Titans: Learning to Memorize at Test Time (2025)
- Nested Learning (Hinton, 2025)
- Toolformer: Language Models Teach Themselves to Use Tools (2023)
- Distilling Step-by-Step! (ACL Findings, 2023)

NOTE: Some tests require legacy modules that have been archived.
These tests will be skipped if the legacy modules are unavailable.
"""

# Import blueprint components
import sys
from pathlib import Path

import pytest

# Torch is optional - skip tests if not available
try:
    import torch

    TORCH_AVAILABLE = True
except ImportError:
    torch = None  # type: ignore
    TORCH_AVAILABLE = False

# Add src to path for imports
src_path = Path(__file__).parent.parent.parent / "src"
legacy_path = Path(__file__).parent.parent.parent / "legacy"
sys.path.insert(0, str(src_path))
sys.path.insert(0, str(legacy_path))

# Check for legacy module availability
try:
    from memory.models import TitansMemory  # noqa: F401

    LEGACY_MEMORY_AVAILABLE = True
except ImportError:
    LEGACY_MEMORY_AVAILABLE = False

try:
    from output.articulation import ChainOfThoughtEnforcer  # noqa: F401

    LEGACY_OUTPUT_AVAILABLE = True
except ImportError:
    LEGACY_OUTPUT_AVAILABLE = False

try:
    from inference.thinking import ToolformerParser  # noqa: F401

    TOOLFORMER_AVAILABLE = True
except ImportError:
    TOOLFORMER_AVAILABLE = False


@pytest.mark.skipif(not TORCH_AVAILABLE, reason="torch not installed")
@pytest.mark.skipif(not LEGACY_MEMORY_AVAILABLE, reason="Legacy memory module archived")
class TestTitansMemorySurprise:
    """
    Test TitansMemory's surprise-driven update mechanism.

    Key verification: High surprise (prediction error) should trigger
    stronger memory updates, as specified in Titans (2025).

    NOTE: These tests use the legacy TitansMemory implementation.
    For v3, use hippocampus.titans.TitansSidecar instead.
    """

    def test_memory_imports(self):
        """Verify TitansMemory can be imported."""
        from memory.models import TitansMemory

        assert TitansMemory is not None

    def test_memory_initialization(self):
        """Test TitansMemory initializes with correct dimensions."""
        from memory.models import TitansMemory

        memory = TitansMemory(
            input_dim=256,
            hidden_dim=512,
            output_dim=256,
            num_heads=4,
        )

        assert memory.input_dim == 256
        assert memory.hidden_dim == 512
        assert memory.num_heads == 4

    def test_surprise_computation(self):
        """
        Test that surprise is computed as prediction error.

        Higher surprise = larger difference between predicted and actual.
        """
        from memory.models import TitansMemory

        memory = TitansMemory(input_dim=64, hidden_dim=128, output_dim=64)

        # Create test tensors
        predicted = torch.randn(1, 64)

        # Actual close to predicted = low surprise
        actual_close = predicted + torch.randn(1, 64) * 0.1
        surprise_low = memory.compute_surprise(predicted, actual_close)

        # Actual far from predicted = high surprise
        actual_far = predicted + torch.randn(1, 64) * 2.0
        surprise_high = memory.compute_surprise(predicted, actual_far)

        # High surprise should be greater
        assert surprise_high > surprise_low, (
            f"High surprise ({surprise_high:.4f}) should exceed "
            f"low surprise ({surprise_low:.4f})"
        )

    def test_memory_update_triggered_by_surprise(self):
        """
        Test that memory updates proportional to surprise signal.

        This is the core Titans mechanism: use surprise as the
        learning signal for test-time memory updates.
        """
        from memory.models import TitansMemory

        memory = TitansMemory(
            input_dim=64,
            hidden_dim=128,
            output_dim=64,
            learning_rate=0.1,
        )

        # Store initial state
        initial_state = memory.get_memory_state()

        # Create high-surprise input
        x = torch.randn(1, 64)
        target = torch.randn(1, 64) * 5.0  # Far from any prediction

        # Update memory with high surprise
        memory.update_memory(x, target, surprise_factor=1.0)

        # Memory should have changed
        new_state = memory.get_memory_state()

        # Verify memory was updated
        assert not torch.allclose(
            initial_state["memory_weights"],
            new_state["memory_weights"],
            atol=1e-6,
        ), "Memory weights should change after surprise-driven update"

    def test_conversation_memory_accumulation(self):
        """
        Simulate a conversation and verify memory accumulates context.

        Scenario: Process multiple turns, verify memory state evolves.
        """
        from memory.models import TitansMemory

        memory = TitansMemory(input_dim=64, hidden_dim=128, output_dim=64)

        # Simulate conversation turns
        num_turns = 5
        memory_states = []

        for _turn in range(num_turns):
            # Simulate input embedding
            x = torch.randn(1, 64)

            # Process through memory
            output = memory.forward(x)

            # Simulate surprise (prediction error)
            actual = torch.randn(1, 64)
            surprise = memory.compute_surprise(output, actual)

            # Update memory based on surprise
            if surprise > 0.5:  # Threshold for update
                memory.update_memory(x, actual, surprise_factor=surprise)

            memory_states.append(memory.get_memory_state()["memory_weights"].clone())

        # Verify memory evolved across turns
        for i in range(1, len(memory_states)):
            # At least some turns should show memory evolution
            if not torch.allclose(memory_states[i], memory_states[i - 1], atol=1e-6):
                return  # Found evolution, test passes

        # If no evolution detected, that's also valid (low surprise throughout)
        assert True


@pytest.mark.skipif(not TORCH_AVAILABLE, reason="torch not installed")
class TestFastSlowWeightManager:
    """
    Test FastSlowWeightManager for nested learning dynamics.

    Key verification: Fast weights update rapidly, slow weights consolidate.
    """

    def test_weight_manager_imports(self):
        """Verify FastSlowWeightManager can be imported."""
        from learning.nested import FastSlowWeightManager

        assert FastSlowWeightManager is not None

    def test_timescale_separation(self):
        """
        Test that fast and slow update intervals are properly separated.
        """
        from learning.nested import FastSlowConfig, FastSlowWeightManager

        config = FastSlowConfig(
            fast_update_interval=10,
            slow_update_interval=100,
        )
        manager = FastSlowWeightManager(config=config)

        assert manager.config.fast_update_interval == 10
        assert manager.config.slow_update_interval == 100
        assert manager.config.fast_update_interval < manager.config.slow_update_interval

    def test_record_interaction(self):
        """
        Test that interactions are recorded and tracked correctly.
        """
        from learning.nested import FastSlowWeightManager

        manager = FastSlowWeightManager()

        result = manager.record_interaction(
            input_text="Test input",
            output_text="Test output",
            quality_score=0.8,
        )

        assert "interaction_count" in result
        assert result["interaction_count"] == 1
        assert "action" in result

    def test_get_update_batch(self):
        """
        Test that update batches can be retrieved.
        """
        from learning.nested import FastSlowWeightManager

        manager = FastSlowWeightManager()

        # Record some interactions
        for i in range(5):
            manager.record_interaction(
                input_text=f"Input {i}",
                output_text=f"Output {i}",
                quality_score=0.9,
            )

        # Get fast update batch
        batch = manager.get_fast_update_batch()
        assert isinstance(batch, list)


@pytest.mark.skipif(not TOOLFORMER_AVAILABLE, reason="ToolformerParser not available")
class TestToolformerParser:
    """
    Test ToolformerParser for autonomous tool-call detection.

    Key verification: Parse tool tokens, execute calls, augment responses.
    """

    def test_parser_imports(self):
        """Verify ToolformerParser can be imported."""
        from inference.thinking import ToolformerParser

        assert ToolformerParser is not None

    def test_tool_call_parsing(self):
        """Test parsing of tool calls from text."""
        from inference.thinking import ToolformerParser

        parser = ToolformerParser()

        text = "The result is [Calculator:15+27] which gives us the answer."
        calls = parser.parse_tool_calls(text)

        assert len(calls) > 0, "Should detect Calculator tool call"
        assert calls[0]["name"] == "Calculator"
        assert "15+27" in calls[0]["args"]

    def test_multiple_tool_calls(self):
        """Test parsing multiple tool calls."""
        from inference.thinking import ToolformerParser

        parser = ToolformerParser()

        text = """
        First, let me [Search:capital of France] and then
        calculate [Calculator:1000/4] to get the result.
        """

        calls = parser.parse_tool_calls(text)

        tool_names = [c["name"] for c in calls]
        assert "Search" in tool_names or "Calculator" in tool_names

    def test_augmentation_format(self):
        """Test that tool results are correctly augmented into text."""
        from inference.thinking import ToolformerParser

        parser = ToolformerParser()

        original_text = "Calculate [Calculator:2+2] for me."
        tool_results = {"Calculator:2+2": "4"}

        augmented = parser.augment_with_tool_results(original_text, tool_results)

        assert "4" in augmented, "Result should be included in augmented text"


@pytest.mark.skipif(not LEGACY_OUTPUT_AVAILABLE, reason="Legacy output module archived")
class TestChainOfThoughtEnforcer:
    """
    Test ChainOfThoughtEnforcer for reasoning extraction.

    Key verification: Extract and structure intermediate reasoning steps.

    NOTE: These tests use the legacy articulation implementation.
    """

    def test_enforcer_imports(self):
        """Verify ChainOfThoughtEnforcer can be imported."""
        from output.articulation import ChainOfThoughtEnforcer

        assert ChainOfThoughtEnforcer is not None

    def test_cot_prompt_generation(self):
        """Test that CoT prompts include reasoning instructions."""
        from output.articulation import ChainOfThoughtEnforcer

        enforcer = ChainOfThoughtEnforcer()

        question = "What is 15% of 80?"
        prompt = enforcer.create_cot_prompt(question)

        # Should include reasoning instruction
        assert any(
            marker in prompt.lower() for marker in ["step by step", "think", "reason", "let's"]
        ), "CoT prompt should include reasoning instruction"

    def test_reasoning_extraction(self):
        """Test extraction of reasoning steps from response."""
        from output.articulation import ChainOfThoughtEnforcer

        enforcer = ChainOfThoughtEnforcer()

        response = """
        <think>
        First, I need to find 15% of 80.
        15% means 15/100 = 0.15
        0.15 * 80 = 12
        </think>
        The answer is 12.
        """

        steps = enforcer.extract_reasoning_steps(response)

        # Should extract reasoning content
        assert len(steps) > 0 or "think" in str(steps).lower()

    def test_enforce_cot_adds_structure(self):
        """Test that enforce_cot adds proper structure to response."""
        from output.articulation import ChainOfThoughtEnforcer

        enforcer = ChainOfThoughtEnforcer()

        structured = enforcer.format_structured_reasoning(
            question="What is the capital of France?",
            reasoning="Historical analysis shows Paris became the capital in the medieval period.",
            answer="Paris",
        )

        assert "Paris" in structured


class TestDistillationPipeline:
    """
    Test DistillationPipeline for knowledge transfer.

    Key verification: CoT distillation, tool augmentation, self-distillation.
    """

    def test_pipeline_imports(self):
        """Verify DistillationPipeline can be imported."""
        from learning.fine_tuning import DistillationConfig, DistillationPipeline

        assert DistillationPipeline is not None
        assert DistillationConfig is not None

    def test_sample_format(self):
        """Test DistillationSample training format."""
        from learning.fine_tuning import DistillationSample

        sample = DistillationSample(
            input_text="What is 2+2?",
            output_text="4",
            rationale="Adding 2 and 2 gives 4",
            tool_calls=[{"name": "Calculator", "args": "2+2"}],
            tool_results=[{"output": "4"}],
            quality_score=0.9,
        )

        training_format = sample.to_training_format()

        assert "instruction" in training_format
        assert "output" in training_format
        assert "What is 2+2?" in training_format["instruction"]

    def test_quality_filtering(self):
        """Test that low-quality samples are filtered."""
        from learning.fine_tuning import DistillationConfig, DistillationPipeline

        config = DistillationConfig(min_quality_score=0.7)
        pipeline = DistillationPipeline(config=config)

        # Create samples with varying quality
        from learning.fine_tuning import DistillationSample

        samples = [
            DistillationSample(input_text="Q1", output_text="A1", quality_score=0.9),
            DistillationSample(input_text="Q2", output_text="A2", quality_score=0.5),
            DistillationSample(input_text="Q3", output_text="A3", quality_score=0.8),
        ]

        filtered = pipeline._filter_samples(samples)

        # Should filter out the 0.5 quality sample
        assert len(filtered) == 2
        assert all(s.quality_score >= 0.7 for s in filtered)


@pytest.mark.skipif(not TORCH_AVAILABLE, reason="torch not installed")
class TestIntegratedWorkflow:
    """
    End-to-end integration tests simulating the full blueprint workflow.
    """

    def test_memory_to_distillation_pipeline(self):
        """
        Test flow from memory update â†’ distillation sample collection.

        Scenario:
        1. Process conversation turn through TitansMemory
        2. High surprise triggers memory update
        3. Quality response becomes distillation candidate
        """
        # Skip if TitansMemoryTorch is not available
        try:
            from memory.models import TitansMemoryTorch
        except ImportError:
            pytest.skip("TitansMemoryTorch not available")

        from learning.fine_tuning import DistillationSample

        # Initialize components
        memory = TitansMemoryTorch(input_dim=64, memory_dim=128)

        # Simulate conversation
        input_embedding = torch.randn(1, 64)

        # Process through memory (returns tuple: output, hidden_state)
        memory_output, _ = memory.forward(input_embedding)

        # Simulate model response and actual result
        actual_result = torch.randn(1, 64)

        # Compute surprise (using MSE as proxy)
        surprise = torch.nn.functional.mse_loss(memory_output, actual_result).item()

        # Create distillation sample
        sample = DistillationSample(
            input_text="Test conversation turn",
            output_text="Model response with reasoning",
            quality_score=max(0.0, 1.0 - surprise),
        )

        assert sample.quality_score >= 0

    def test_tool_augmentation_workflow(self):
        """
        Test Toolformer-style augmentation workflow.

        Scenario:
        1. Parse tool calls from response
        2. Execute tools
        3. Augment response with results
        4. Score augmentation quality
        """
        # Skip - ToolformerParser is not implemented in current version
        try:
            from inference.thinking import ToolformerParser
        except (ImportError, AttributeError):
            pytest.skip("ToolformerParser not available in current version")

        from learning.fine_tuning import DistillationSample

        parser = ToolformerParser()

        # Original response with tool call
        original = "To find the area, calculate [Calculator:pi*5*5] square units."

        # Parse tool calls
        calls = parser.parse_tool_calls(original)

        if calls:
            # Simulate tool execution
            tool_results = {}
            for call in calls:
                key = f"{call['name']}:{call['args']}"
                tool_results[key] = "78.54"  # Simulated result

            # Augment response
            augmented = parser.augment_with_tool_results(original, tool_results)

            # Create distillation sample with tool augmentation
            sample = DistillationSample(
                input_text="What is the area of a circle with radius 5?",
                output_text=augmented,
                tool_calls=calls,
                tool_results=[{"output": v} for v in tool_results.values()],
                quality_score=0.85,
            )

            training_data = sample.to_training_format()
            assert "Calculator" in str(training_data) or len(training_data) > 0


# Pytest markers for different test categories
pytestmark = [
    pytest.mark.integration,
]


if __name__ == "__main__":
    # Run tests
    pytest.main([__file__, "-v", "--tb=short"])
